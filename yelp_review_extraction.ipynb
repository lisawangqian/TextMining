{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Author: Qian Wang\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yelpapi\n",
    "import rauth\n",
    "import time\n",
    "import json\n",
    "import io\n",
    "import codecs\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib import urlopen\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "import re\n",
    "\n",
    "import string\n",
    "import sys  \n",
    "\n",
    "#reload(sys)  \n",
    "#sys.setdefaultencoding('utf8')\n",
    "\n",
    "last_fetched_at = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Review_Extraction():\n",
    "    def __init__(self, location=(40.7311019, -74.00137210000003), limit=20, radius_filter=1000,count=0,index=0):\n",
    "\n",
    "        self.location = location\n",
    "        self.limit = limit\n",
    "        self.radius_filter = radius_filter\n",
    "        self.index = index\n",
    "        self.count = count\n",
    "    def get_search_parameters(self, latitude, longitude, radius_filter, limit):\n",
    "        # See the Yelp API for more details\n",
    "        params = {}\n",
    "        params[\"term\"] = \"restaurant\"\n",
    "        params[\"ll\"] = \"{},{}\".format(str(latitude), str(longitude))\n",
    "        params[\"radius_filter\"] = str(radius_filter)\n",
    "        params[\"limit\"] = str(limit)\n",
    "\n",
    "        return params\n",
    "\n",
    "    def get_results(self, params):\n",
    "\n",
    "        # Obtain these from Yelp's manage access page\n",
    "        consumer_key = \"qvtAmgqN2IgX1-x54gTQZw\"\n",
    "        consumer_secret = \"_9YP_S2lsrT0kcVzGVKc--SM7Hw\"\n",
    "        token = \"4dl-TfK8eitmcztp_H8CMznEb2z_r6w7\"\n",
    "        token_secret = \"mC1jEXAi8jlP3lzsRIWgBcR6J3o\"\n",
    "\n",
    "        session = rauth.OAuth1Session(\n",
    "            consumer_key=consumer_key\n",
    "            , consumer_secret=consumer_secret\n",
    "            , access_token=token\n",
    "            , access_token_secret=token_secret)\n",
    "\n",
    "        request = session.get(\"http://api.yelp.com/v2/search\", params=params)\n",
    "\n",
    "        # Transforms the JSON API response into a Python dictionary\n",
    "        data = request.json()\n",
    "        session.close()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def findRestaurant(self):\n",
    "\n",
    "        lati, longi = self.location\n",
    "        params = self.get_search_parameters(lati, longi, self.radius_filter, self.limit)\n",
    "        result = self.get_results(params)\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        with io.open('restaurant_data_big'+str(self.index)+'.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(unicode(json.dumps(result, ensure_ascii=False)))\n",
    "        f.close()\n",
    "\n",
    "        totalBusiness = result[\"total\"]\n",
    "        BizList = result[\"businesses\"]\n",
    "\n",
    "        columns = [\"name\", \"is_claimed\", \"review_count\", \"url\", \"rating\", \"city\"]\n",
    "        data_df = pd.DataFrame(columns=columns)\n",
    "        i = 0\n",
    "        for each in BizList:\n",
    "            isClaim = each[\"is_claimed\"]\n",
    "            rating = each[\"rating\"]\n",
    "            name = each[\"name\"]\n",
    "            url = each[\"url\"]\n",
    "            revCnt = each[\"review_count\"]\n",
    "            city = each[\"location\"][\"city\"]\n",
    "            row = [name, isClaim, revCnt, url, rating, city]\n",
    "            data_df.loc[i] = row\n",
    "            i += 1\n",
    "\n",
    "        return data_df, totalBusiness, i\n",
    "\n",
    "    def get_single_review(self, url):\n",
    "        global last_fetched_at\n",
    "\n",
    "        wait_interval = 2000\n",
    "        if last_fetched_at is not None:\n",
    "            now = time.time()\n",
    "            elapsed = now - last_fetched_at\n",
    "            if elapsed < wait_interval:\n",
    "                time.sleep((wait_interval - elapsed) / 1000)\n",
    "\n",
    "        page = urlopen(url)\n",
    "        soup = bs(page, \"lxml\")\n",
    "        reviews = soup.findAll('p', attrs={'itemprop': 'description'})\n",
    "        authors = soup.findAll('span', attrs={'itemprop': 'author'})\n",
    "        flag = True\n",
    "        indexOf = 1\n",
    "        content = \"\"\n",
    "        nlp_content = \"\"\n",
    "        for review in reviews:\n",
    "            dirtyEntry = str(review)\n",
    "            while dirtyEntry.index('<') != -1:\n",
    "                indexOf = dirtyEntry.index('<')\n",
    "                endOf = dirtyEntry.index('>')\n",
    "                if flag:\n",
    "                    dirtyEntry = dirtyEntry[endOf + 1:]\n",
    "                    flag = False\n",
    "                else:\n",
    "                    if (endOf + 1 == len(dirtyEntry)):\n",
    "                        cleanEntry = dirtyEntry[0:indexOf]\n",
    "                        break\n",
    "                    else:\n",
    "                        dirtyEntry = dirtyEntry[0:indexOf] + dirtyEntry[endOf + 1:]\n",
    "\n",
    "            content += cleanEntry\n",
    "\n",
    "            eachEntry = cleanEntry.decode(encoding='UTF-8', errors='ignore')\n",
    "            lowers_entry = eachEntry.lower()\n",
    "\n",
    "            no_punctuation_entry = re.sub(r'[^\\w\\s]', '', lowers_entry)\n",
    "            tokens_entry = nltk.word_tokenize(no_punctuation_entry)\n",
    "            filtered_entry = [w for w in tokens_entry if not w in stopwords.words('english')]\n",
    "            stemmer = PorterStemmer()\n",
    "            for item in filtered_entry:\n",
    "                nlp_content += (stemmer.stem(item) + \" \")\n",
    "\n",
    "        last_fetched_at = time.time()\n",
    "\n",
    "        return content, nlp_content\n",
    "\n",
    "    def get_reviews(self, url_list, filename1=\"review_whole_big\", filename2=\"review_clean_big\"):\n",
    "\n",
    "        rec_len = len(url_list)\n",
    "        with open(filename1+str(self.index)+\".txt\", 'w') as f:\n",
    "            # for each in url_list:\n",
    "            # print str(i) + \" \" + self.get_single_review(each)\n",
    "            f.writelines((str(i+self.count) + \" \" + self.get_single_review(url_list[i])[0] + \"\\n\") for i in range(rec_len - 1))\n",
    "            f.writelines((str(self.count+rec_len - 1) + \" \" + self.get_single_review(url_list[rec_len - 1])[0]))\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        with open(filename2+str(self.index)+\".txt\", 'w') as f:\n",
    "            # for each in url_list:\n",
    "            # print str(i) + \" \" + self.get_single_review(each)\n",
    "            f.writelines((str(i+self.count) + \" \" + self.get_single_review(url_list[i])[1] + \"\\n\") for i in range(rec_len - 1))\n",
    "            f.writelines((str(self.count+rec_len - 1) + \" \" + self.get_single_review(url_list[rec_len - 1])[1]))\n",
    "\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832\n",
      "693\n",
      "1231\n",
      "513\n",
      "113\n",
      "355\n",
      "278\n",
      "107\n",
      "58\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    locations = [(40.7311019, -74.00137210000003), (40.7167219, -73.99814459999999), (40.758895, -73.98513100000002), (40.706441, -74.0052976), (40.7794366, -73.96324400000003), (41.8773933, -87.6256477), (41.8942258,-87.61980819999997), (41.92979379999999, -87.65516830000001), (41.79611199999999, -87.58862899999997), (41.8728752, -87.65401689999999)]\n",
    "    index = 0\n",
    "    count = 0\n",
    "    i = 0\n",
    "    for location in locations:\n",
    "        rev_ext = Review_Extraction(location,index=index,count=count)\n",
    "        dftmp, numBiztmp, i = rev_ext.findRestaurant()\n",
    "        print numBiztmp\n",
    "        dftmp.to_csv(\"data_feature_big\"+str(index)+\".csv\", encoding='utf8')\n",
    "        rev_ext.get_reviews(dftmp['url'])\n",
    "        count = count+i\n",
    "        index = index+1\n",
    "        time.sleep(1.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
